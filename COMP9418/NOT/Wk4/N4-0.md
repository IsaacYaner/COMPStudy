# Week 4

## Markov Chains and Hidden Markov Models

+ Dynamic Bayesian Networks (DBN)
  + Markov chain
  + Hidden Markov Models(HMM)
+ Markov property
  + independent of past given current

## Markov chain

+ State **machine**
+ Parameters
  + $P(X_1)$
  + $P(X_t|X_{t-1})**$**
  + $|X|+(n-1)|X|^2$ parameters
+ Transition probabilities doesn't change

### Probability

+ $P(X_1,X_2,X_3)=P(X_1)*P(X_2|X_1)*P(X_3|X_2)$
+ $P(S^d_s) = P(s|s)^{d-1}(1-P(s|s))$
  + Staying in a state for d steps
+ $\mathbb E(s)=\frac{1}{1-P(s|s)}$
  + expected duration in a state

### Features

+ Stationary distribution
  + chain converge
  + $\pi$
  + Every markov chain has at least one
    + **Aperiodic** and **Irreducible** converges unique
+ Irreducible
  + *regular* or *ergodic*
  + any state is reachable from any state
  + chain is recurrent
    + every state can be reached infinite times
+ Aperiodicity
  + can return to state itself at any time

## Hidden Markov Models (HMM)

+ **parameters**
  + Initial distribution $P(X_1)$
  + Transition probabilities
  + Emission probabilities

### Inference

+ $B(X) = P(X|e_1, ... , e_t)$
  + *belief of state*
  + Steps
    + Passage of time
    + Observation

### Viterbi

+ Input
  + $n,T,E,P(X_1),\{e\}$
+ $m[x,1] = logP(X_1=x)$
+ $for\ t$
  + $for\ each\ state\ x_t$
    + $m[x_t, t]=-\infin$
    + $for\ each\ state\ x_{t-1}$
      + $if\ m[x_{t-1},t-1]+logT(x_t|x_{t-1})>m[x_t,t]$
        + $m[x_t,t] = m[x_t-1,t-1]+logT(x_t,x_{t-1})$
    + $m[x_t,t] = m[x_t,t]+logE(e_t|x_t)$
+ $return\ p(x,n)$

## Question

+ Chain rule
