# Probability Calculus

## Keywords

+ Framework for reasoning with **uncertain beliefs**
+ Degree of belief (**DoB**)
+ Notion of independence

## Overview

+ In addition to propositional logic
  + Use **DoB** to replace **0** and **1**
+ $P(\alpha)=\Sigma_{w\vDash \alpha} P(w)$

## Uncertainty

### Entropy

+ $H(X) = -\Sigma P(X)log_2P(X)$
+ maximises when $p = \frac{1}{2}$

## Updating Belief  

+ $P(·|\beta)$
+ $P(\alpha|\beta) = \frac{P(\alpha\land \beta)}{P(\beta)}$
  + $P(w|\beta) = 0$ if $w\vDash \lnot \beta$
  + $P(w|\beta)=\frac{P(w)}{P(\beta)}$  


### Conditional Entropy

+ $H(X|Y)=\Sigma_yP(y)H(H|y)$
+ $H(X|Y)\le H(X)$

### Independence

+ $P(\alpha | \beta) = P(\alpha)$
+ $P(a\land b) = P(a)P(b)$

#### Conditional Independence

+ Independence is dynamic given conditions
+ $P(a|b\land c) = P(a|c)$
+ $P(a\land b|c) = P(a|c)P(b|c)$
+ Symmetric
  + $a\ ind\ b,c\iff b\ ind\ a,c$

#### Variable independence

+  $X⊥Y|Z$
   +  X is independent of Y given Z is denoted by 

### Mutual Information

+ $MI(X;Y)=\Sigma_{x,y}P(x,y)log_2\frac{P(x,y)}{P(x)P(y)}$
  + Non-negative
  + Equal to zero only if X and Y are independent
    + $MI(X;Y)=H(X)-H(X|Y)$
    + $MI(X;Y)=H(Y)-H(Y|X)$

#### Conditional Mutual Information

+ $MI(X;Y|Z)=\Sigma_{x,y,z}P(x,y,z)log_2\frac{P(x,y|z)}{P(x|z)P(y|z)}$
+ $MI(X;Y|Z)=H(X|Z)-H(X|Y,Z)$
+ $MI(X;Y|Z)=H(Y|Z)-H(Y|X,Z)$
