{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9418 - Assignment 1 - Bayesian Networks as Classifiers\n",
    "\n",
    "## UNSW Sydney, September 2021\n",
    "\n",
    "- Haodong Lu - Z5183944\n",
    "- Yiyan Yang - Z5183946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "**Submission deadline:** Sunday, 17th October 2021, at 18:00:00.\n",
    "\n",
    "**Late Submission Policy:** The penalty is set at 20% per late day. This is ceiling penalty, so if a group is marked 60/100 and they submitted two days late, they still get 60/100.\n",
    "\n",
    "**Form of Submission:** This is a group assignment. Each group can have up to **two** students. **Only one member of the group should submit the assignment**.\n",
    "\n",
    "You can reuse any piece of source code developed in the tutorials.\n",
    "\n",
    "Submit your files using give. On a CSE Linux machine, type the following on the command-line:\n",
    "\n",
    "``$ give cs9418 ass1 solution.zip``\n",
    "\n",
    "Alternative, you can submit your solution via [WebCMS](https://webcms3.cse.unsw.edu.au/COMP9418/21T3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical prerequisites\n",
    "\n",
    "These are the libraries your are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allowed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import heapq as pq\n",
    "import matplotlib as mp\n",
    "import math\n",
    "from itertools import product, combinations\n",
    "from graphviz import Digraph\n",
    "from tabulate import tabulate\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the python files we developed in tutorials, or any other code from the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DiscreteFactors import Factor\n",
    "from Graph import Graph\n",
    "from BayesNet import allEqualThisIndex, estimateFactor, BayesNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial task - Initialise graph\n",
    "\n",
    "Create a graph ``G`` that represents the following network by filling in the edge lists.\n",
    "![Bayes Net](BayesNet.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Graph({\n",
    "    \"BreastDensity\" : [\"Mass\"],\n",
    "    \"Location\" : [\"BC\"],\n",
    "    \"Age\" : [\"BC\"],\n",
    "    \"BC\" : [\"Metastasis\", \"MC\", \"SkinRetract\", \"NippleDischarge\", \"AD\", \"Mass\"],\n",
    "    \"Mass\" : [\"Size\", \"Shape\", \"Margin\"],\n",
    "    \"AD\" : [\"FibrTissueDev\"],\n",
    "    \"Metastasis\" : [\"LymphNodes\"],\n",
    "    \"MC\" : [],\n",
    "    \"Size\" : [],\n",
    "    \"Shape\" : [],\n",
    "    \"FibrTissueDev\" : [\"SkinRetract\", \"NippleDischarge\", \"Spiculation\"],\n",
    "    \"LymphNodes\" : [],\n",
    "    \"SkinRetract\" : [],\n",
    "    \"NippleDischarge\" : [],\n",
    "    \"Spiculation\" : [\"Margin\"],\n",
    "    \"Margin\" : [],\n",
    "})\n",
    "\n",
    "# G.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('bc.csv') as file:\n",
    "    data = pd.read_csv(file)\n",
    "\n",
    "#remove 2 variables from data (because we are pretending we don't know this information)\n",
    "if 'Metastasis' in data:\n",
    "    del data['Metastasis']\n",
    "if 'LymphNodes' in data:\n",
    "    del data['LymphNodes']\n",
    "\n",
    "# remove same 2 nodes from graph\n",
    "G.remove_node('Metastasis')\n",
    "G.remove_node('LymphNodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 1 - Efficient d-separation test\n",
    "\n",
    "Implement the efficient version of the d-separation algorithm in a function ``d_separation(G, X, Z, Y)`` that return a boolean: ``True`` if **X** is d-separated from **Y** given **Z** in the graph $G$ and ``False`` otherwise.\n",
    "\n",
    "* **X**,**Y** and **Z** are python sets, each containing a set of variable names. \n",
    "* Variable names may be strings or integers, and can be assumed to be nodes of the graph $G$. \n",
    "* $G$ is a directed graph object as defined in tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for d_separation(G, X, Z, Y) in one or more cells here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The d-seperation $dsep_G(X,Y,Z)$ is implemented by first repeatedly pruning leaf nodes not belong to $X \\vee Y \\vee Z$ and then pruning all edges outgoing from nodes in $Z$. Then we test the connectivity between $X$ and $Y$ by making an undirected copy of the pruned graph and then using DFS to traverse from the set of nodes in $X$ to the set of nodes in $Y$. If they are connected then $X$ and $Y$ are d-seperated by $Z$, and vise-versa. Note that based on the definition of d-seperation where $dsep_G(X,Y,Z)$ is $True$ iff every path between a node in $X$ and a node in $Y$ is blocked by $Z$, we set that the connectivity check fails if there is one node in $X$ cannot traverse to a node in $Y$ and that would result in $dsep_G(X,Y,Z)$ is $True$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def to_undirected(g):\n",
    "    ug = Graph()\n",
    "    for node in g:\n",
    "        ug.add_node(node)\n",
    "        for edge in g.children(node):\n",
    "            ug.add_edge(node, edge, directed=False)\n",
    "    return ug\n",
    "\n",
    "def connectivity_check(G, X, Y):\n",
    "    x2y = True\n",
    "    for node_x in X:\n",
    "        for node_y in Y:\n",
    "            if G.dfs(node_x)[node_y] == \"white\":\n",
    "                x2y = False\n",
    "                break\n",
    "    return x2y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_separation(G, X, Z, Y):\n",
    "    '''\n",
    "    Arguments:\n",
    "    G:   is an object of type Graph (the class you developed in tutorial 1)\n",
    "    X,Z and Y:  are python `set` objects.\n",
    "    '''\n",
    "    # 1. We delete any leaf node W from DAG G as long as W does not belong to X ∪ Y ∪ Z. This process is\n",
    "    # repeated until no more nodes can be deleted.\n",
    "    G_copy = copy.deepcopy(G)\n",
    "    union = X.union(Y,Z)\n",
    "\n",
    "    pruned = False\n",
    "    while not pruned:\n",
    "        pruned = True\n",
    "        leaf_nodes = []\n",
    "        for node in G_copy:\n",
    "            if len(G_copy.children(node)) == 0:\n",
    "                leaf_nodes.append(node)\n",
    "        for leaf in leaf_nodes:\n",
    "            if leaf not in union:\n",
    "                pruned = False\n",
    "                G_copy.remove_node(leaf)\n",
    "\n",
    "    # 2. We delete all edges outgoing from nodes in Z\n",
    "    for node in Z:\n",
    "        G_copy.adj_list[node] = []\n",
    "\n",
    "\n",
    "    # 3. Check connectivity\n",
    "    return not connectivity_check(to_undirected(G_copy), X, Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "## Note: More hidden tests will be used. You should make more tests yourself.\n",
    "\n",
    "def test(statement):\n",
    "    if statement:\n",
    "        print(\"Passed test case\")\n",
    "    else:\n",
    "        print(\"Failed test case\")\n",
    "        \n",
    "test(d_separation(G, set(['Age']), set(['BC']), set(['AD'])))\n",
    "test(not d_separation(G, set(['Spiculation','SkinRetract']), set(['MC', 'Size']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "# my tests\n",
    "\n",
    "# test sequential true\n",
    "test(d_separation(G, set(['BC']),  set(['Mass']), set(['Size'])))\n",
    "test(d_separation(G, set(['BC']), set(['AD']), set(['FibrTissueDev'])))\n",
    "test(d_separation(G, set(['BC']), set(['Mass','AD']), set(['Margin','FibrTissueDev'])))\n",
    "# test sequential false\n",
    "test(not d_separation(G, set(['BC']),  set(['Mass']), set(['Margin'])))\n",
    "\n",
    "# test divergent true\n",
    "test(d_separation(G, set(['Mass']),  set(['BC']), set(['SkinRetract'])))\n",
    "test(d_separation(G, set(['NippleDischarge']),  set(['FibrTissueDev', 'BC']), set(['SkinRetract'])))\n",
    "# test divergent false\n",
    "test(not d_separation(G, set(['NippleDischarge']),  set(['FibrTissueDev']), set(['SkinRetract'])))\n",
    "\n",
    "# test convergent true\n",
    "test(d_separation(G, set(['Mass']), set(['BC']), set(['Spiculation'])))\n",
    "# test convergent false\n",
    "test(not d_separation(G, set(['Mass']),  set(['BC', 'Margin']), set(['Spiculation'])))\n",
    "test(not d_separation(G, set(['Location']),  set(['BC']), set(['Age'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 2 - Estimate Bayesian Network parameters from data\n",
    "\n",
    "Implement a function ``learn_outcome_space(data)`` that learns the outcome space (the valid values for each variable) from the pandas dataframe ``data`` and returns a dictionary ``outcomeSpace`` with these values.\n",
    "\n",
    "Implement a method ``model.learn_parameters(data, alpha=1)`` that learns the parameters of the Bayesian Network `model`. This function should do the same as the ``learn_parameters`` function from tutorials, but it should also implement laplacian smoothing with parameter $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_outcome_space(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_outcome_space(data):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data - A pandas dataframe\n",
    "    Returns: \n",
    "        outcomeSpace - A dictionary. e.g. {'A':('True', 'False'), 'B':('up','down','left'), 'C':(1,2,3,4)}\n",
    "    '''\n",
    "    outcomeSpace = {}\n",
    "    for key in data:\n",
    "        outcomeSpace[key] = tuple(set(data[key]))\n",
    "\n",
    "    return outcomeSpace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "outcomeSpace = learn_outcome_space(data)\n",
    "\n",
    "outcomes = outcomeSpace['BreastDensity']\n",
    "answer = ('high', 'medium', 'low')\n",
    "test(len(outcomes) == len(answer) and set(outcomes) == set(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learnParameters in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def estimateFactor_alpha(data, var_name, parent_names, outcomeSpace, alpha=1, log=False):\n",
    "    var_outcomes = outcomeSpace[var_name]\n",
    "    parent_outcomes = [outcomeSpace[var] for var in (parent_names)]\n",
    "    # cartesian product to generate a table of all possible outcomes\n",
    "    all_parent_combinations = product(*parent_outcomes)\n",
    "\n",
    "    # calculating the number of values for X (|X|)\n",
    "    mod_X = len(var_outcomes)\n",
    "    for p in parent_outcomes:\n",
    "        mod_X *= len(p)\n",
    "\n",
    "    f = Factor(list(parent_names)+[var_name], outcomeSpace)\n",
    "    \n",
    "    for i, parent_combination in enumerate(all_parent_combinations):\n",
    "        parent_vars = dict(zip(parent_names, parent_combination))\n",
    "        parent_index = allEqualThisIndex(data, **parent_vars)\n",
    "        for var_outcome in var_outcomes:\n",
    "            var_index = (np.asarray(data[var_name])==var_outcome)\n",
    "            if log:\n",
    "                f[tuple(list(parent_combination)+[var_outcome])] = np.log2(((var_index & parent_index).sum()+alpha)/(parent_index.sum() + alpha*mod_X))\n",
    "            else:\n",
    "                f[tuple(list(parent_combination)+[var_outcome])] = ((var_index & parent_index).sum()+alpha)/(parent_index.sum() + alpha*mod_X)\n",
    "        \n",
    "            \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(BayesNet):\n",
    "    def learn_parameters(self, data, alpha=1, log=False):\n",
    "        graphT = self.graph.transpose()\n",
    "        for node, parents in graphT.adj_list.items():\n",
    "            f = estimateFactor_alpha(data, node, parents, self.outcomeSpace, alpha, log)\n",
    "            self.factors[node] = f\n",
    "            \n",
    "model = BayesNet(graph=G, outcomeSpace=outcomeSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "0.248000399920016\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "model.learn_parameters(data, alpha=1)\n",
    "\n",
    "test(model.factors['Age']['35-49'] == 0.248000399920016)\n",
    "print(model.factors['Age']['35-49'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 3 - Bayesian Network Classification\n",
    "\n",
    "Design a new function ``assess_bayes_net(model, dataframe, var)`` that uses the test cases in ``dataframe`` to assess the performance of the Bayesian network at classifying the variable `var`. Implement the efficient classification procedure discussed in the lectures (make sure you understand what a Markov Blanket is). Such a function should return the classifier accuracy. \n",
    "\n",
    " * ``var`` is the name of the variable you are predicting, using the values of all the other variables. \n",
    " \n",
    "If you like, you can add new functions to the BayesNet class, or write helper functions to help solve the above task.\n",
    "\n",
    "Using another function called `cross_validation_bayes_net`, compute and report the average accuracy over the ten cross-validation runs as well as the standard deviation. A scaffold for this function is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_bayes_net in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesNet(graph=G, outcomeSpace=outcomeSpace)\n",
    "def assess_bayes_net(model, dataframe, var='BC'):\n",
    "    evidence = {}\n",
    "    prob = []\n",
    "    num_keys = dataframe.shape[0]\n",
    "\n",
    "    for i in range(0, num_keys):\n",
    "        for d in dataframe:\n",
    "            if d == var:\n",
    "                continue\n",
    "            evidence[d] = dataframe[d].iloc[i]\n",
    "        probi = model.query([var], **evidence)\n",
    "        tb = probi.table\n",
    "        index = np.argmax(tb)\n",
    "        prob.append(probi.outcomeSpace[probi.domain[0]][index])\n",
    "    \n",
    "    '''\n",
    "    for i in range(0, num_keys):\n",
    "        print(prob[i].table, prob[i].outcomeSpace[prob[i].domain[0]][0])\n",
    "    '''\n",
    "    # List of real var values\n",
    "    true_value = dataframe[var]\n",
    "    # Predicted var values\n",
    "    predict_value = pd.Series(prob)\n",
    "\n",
    "    # Mean accuracy\n",
    "    acc = (((predict_value == true_value)).mean())\n",
    "\n",
    "    return acc\n",
    "\n",
    "def cv_bayes(dataframe, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    for i in range(k):\n",
    "        model = BayesNet(graph=G, outcomeSpace=outcomeSpace)\n",
    "        # split dataset into train and test\n",
    "        \n",
    "        # train a model\n",
    "        start = int(i*dataframe.shape[0]/k)\n",
    "        end = int((i+1)*dataframe.shape[0]/k)\n",
    "        train = dataframe.drop([i for i in range(start, end)], axis=0)\n",
    "        test = dataframe.iloc[start:end]\n",
    "        train = train.reset_index()\n",
    "        test = test.reset_index()\n",
    "        train.drop(columns=[\"index\"])\n",
    "        test.drop(columns=[\"index\"])\n",
    "\n",
    "        # model.learnParameters\n",
    "        model.learn_parameters(train, alpha=1)\n",
    "\n",
    "        # test the model with assess_bayes_net\n",
    "        acc = assess_bayes_net(model, test, var)\n",
    "        \n",
    "        accuracy_list.append(acc)\n",
    "    return accuracy_list\n",
    "\n",
    "def cross_validation_bayes_net(dataframe, var='BC', k=10):\n",
    "    accuracy_list = cv_bayes(dataframe, var, k)\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "acc, stddev = cross_validation_bayes_net(data, 'BC', 10)\n",
    "test(abs(acc - 0.85) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Marks] Task 4 - Naïve Bayes Classification\n",
    "\n",
    "Design a new function ``assess_naive_bayes(model, data, var)`` to classify and assess the test cases in ``data``. To classify each example, use the log probability trick discussed in the lectures. Do $k$-fold cross-validation with the `cross_validation_naive_bayes(data, var, k)` function, same as above, and return ``acc`` and ``stddev``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for assess_naive_bayes(model, data, var) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate a graph for naive bayse network\n",
    "def get_naive_g(outcomeSpace, var=\"BC\"):\n",
    "    naive_g = {\n",
    "        var: []\n",
    "    }\n",
    "    for key in outcomeSpace:\n",
    "        if key != var:\n",
    "            naive_g[var].append(key)\n",
    "            naive_g[key] = []\n",
    "\n",
    "    naive_g = Graph(naive_g)\n",
    "\n",
    "    return naive_g\n",
    "\n",
    "# modified version of tut code to join log probalility tables\n",
    "class Factor(Factor):\n",
    "    def log_join(self, other):\n",
    "        '''\n",
    "        This function multiplies two factors.\n",
    "        '''\n",
    "        # confirm that any shared variables have the same outcomeSpace\n",
    "        for var in set(other.domain).intersection(set(self.domain)):\n",
    "            if self.outcomeSpace[var] != other.outcomeSpace[var]:\n",
    "                raise IndexError('Incompatible outcomeSpaces. Make sure you set the same evidence on all factors')\n",
    "\n",
    "        # extend current domain with any new variables required\n",
    "        new_dom = list(self.domain) + list(set(other.domain) - set(self.domain)) \n",
    "        \n",
    "        # to prepare for multiplying arrays, we need to make sure both arrays have the correct number of axes\n",
    "        self_t = self.table\n",
    "        other_t = other.table\n",
    "        for _ in set(other.domain) - set(self.domain):\n",
    "            self_t = self_t[..., np.newaxis]     \n",
    "        for _ in set(self.domain) - set(other.domain):\n",
    "            other_t = other_t[..., np.newaxis]\n",
    "\n",
    "        # And we need the new axes to be transposed to the correct location\n",
    "        old_order = list(other.domain) + list(set(self.domain) - set(other.domain)) \n",
    "        new_order = []\n",
    "        for v in new_dom:\n",
    "            new_order.append(old_order.index(v))\n",
    "        other_t = np.transpose(other_t, new_order)\n",
    "        \n",
    "        # Now that the arrays are all set up, we can rely on numpy broadcasting to work out which numbers need to be multiplied.\n",
    "        # https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "        new_table = self_t + other_t\n",
    "        \n",
    "        # The final step is to create the new outcomeSpace\n",
    "        new_outcomeSpace = self.outcomeSpace.copy()\n",
    "        new_outcomeSpace.update(other.outcomeSpace)\n",
    "\n",
    "        # in the following line, `self.__class__` is the same as `Factor` (except it doesn't break things when subclassing)\n",
    "        return self.__class__(tuple(new_dom), new_outcomeSpace, table=new_table)\n",
    "\n",
    "# modified version of tut code for query on log probability tables\n",
    "class BayesNet(BayesNet):\n",
    "    def log_query(self, var, **q_evi):\n",
    "        backup_factors = copy.deepcopy(self.factors)\n",
    "        # set evidence on all relevant factors \n",
    "        for key, factor in self.factors.items():\n",
    "            self.factors[key] = factor.evidence(**q_evi)\n",
    "\n",
    "        factor_list = list(self.factors.values())\n",
    "        \n",
    "        accumulator = factor_list[0]\n",
    "        for factor in factor_list[1:]:\n",
    "            accumulator = accumulator.log_join(factor)\n",
    "\n",
    "        for var in self.outcomeSpace:\n",
    "            if var != var:\n",
    "                accumulator = accumulator.marginalize(var)\n",
    "\n",
    "        accumulator.table = np.exp2(accumulator.table)\n",
    "\n",
    "        self.factors = backup_factors\n",
    "        return accumulator.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_naive_bayes(model, dataframe, var='BC'):\n",
    "    evidence = {}\n",
    "    prob = []\n",
    "    num_keys = dataframe.shape[0]\n",
    "\n",
    "    for i in range(0, num_keys):\n",
    "        for d in dataframe:\n",
    "            if d == var:\n",
    "                continue\n",
    "            evidence[d] = dataframe[d].iloc[i]\n",
    "        probi = model.log_query([var], **evidence)\n",
    "        tb = probi.table\n",
    "        index = np.argmax(tb)\n",
    "        prob.append(probi.outcomeSpace[probi.domain[0]][index])\n",
    "\n",
    "    # List of real var values\n",
    "    true_value = dataframe[var]\n",
    "    # Predicted var values\n",
    "    predict_value = pd.Series(prob)\n",
    "\n",
    "    # Mean accuracy\n",
    "    acc = (((predict_value == true_value)).mean())\n",
    "    return acc\n",
    "\n",
    "def cv_naive(dataframe, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    # value init\n",
    "    # note that graph structure is directly learned from data given var\n",
    "    naive_g = get_naive_g(outcomeSpace, var)\n",
    "    # suffle the dataframe\n",
    "    dataframe = dataframe.sample(frac=1)\n",
    "    for i in range(k):\n",
    "        # split dataset into train and test\n",
    "        start = int(i*dataframe.shape[0]/k)\n",
    "        end = int((i+1)*dataframe.shape[0]/k)\n",
    "        train = dataframe.drop([i for i in range(start, end)], axis=0)\n",
    "        test = dataframe.iloc[start:end]\n",
    "        train = train.reset_index()\n",
    "        test = test.reset_index()\n",
    "        train.drop(columns=[\"index\"])\n",
    "        test.drop(columns=[\"index\"])\n",
    "\n",
    "        # train a model\n",
    "        model = BayesNet(graph=naive_g, outcomeSpace=outcomeSpace)\n",
    "\n",
    "        # model.learnParameters\n",
    "        model.learn_parameters(train, alpha=1, log=True)\n",
    "\n",
    "        # test the model with assess_bayes_net\n",
    "        acc = assess_naive_bayes(model, test, var)\n",
    "\n",
    "        accuracy_list.append(acc)\n",
    "    return accuracy_list\n",
    "\n",
    "def cross_validation_naive_bayes(dataframe, var='BC', k=10):\n",
    "    accuracy_list = cv_naive(dataframe, var, k)\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "## TEST CODE\n",
    "\n",
    "acc, stddev = cross_validation_naive_bayes(data, 'BC')\n",
    "test(abs(acc - 0.80) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 5 - Tree-augmented Naïve Bayes Classification\n",
    "\n",
    "Similarly to the previous task, implement a Tree-augmented Naïve Bayes (TAN) classifier and evaluate your implementation in the breast cancer dataset. Design a function ``learn_tan_structure(data, class_var)`` to learn the TAN structure (graph) from ``data`` and return such a structure. Scaffolds for required functions are given below. Implement other helper functions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Develop your code for learn_tan_structure(data) in one or more cells here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo learn tan structure (copied from lec note)\n",
    "# \n",
    "# for i = 1 to n do\n",
    "    # for j = 1 to n do\n",
    "        # m[i,j] <- MI(A_i,A_j|C)\n",
    "# G <- complete undirected graph over {A_1, ..., A_n} with weight m\n",
    "# G_T <- maximal spanning tree for G\n",
    "# G_T^D ← G_T directed by choosing any variable as root and setting edge directions outward from root\n",
    "# G_T^D <- G_T^D with node C added and direct edges from C to each attribute node\n",
    "# Learn parameters for G_T^D\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# helper functions\n",
    "\n",
    "# get mutual information matrix\n",
    "# note that due to the smoothing factor, there will be some values < 0\n",
    "# but these values are very small (close to -0.0011436948893404075) which the \n",
    "# effects can be ignored\n",
    "def get_MI(data, var, outcomeSpace, attributes,alpha=1):\n",
    "\n",
    "    # build the mutual information matrix\n",
    "    m = {}\n",
    "    template = {}\n",
    "    for key in attributes:\n",
    "        template[key] = 0\n",
    "  \n",
    "    for key in attributes:\n",
    "        m[key] = template.copy()\n",
    "    # store all single parent cpt and prior probability to reduce the time of \n",
    "    # duplicate calculating factors\n",
    "    attr_factor = {}\n",
    "    prior = estimateFactor_alpha(data, var, [], outcomeSpace,alpha=alpha)\n",
    "    for i in attributes:\n",
    "        attr_factor[i] = estimateFactor_alpha(data,i, [var], outcomeSpace,alpha=alpha)\n",
    "\n",
    "    # fill in the mutual information matrix\n",
    "    for i in attributes:\n",
    "        for j in attributes:\n",
    "            # print(i, j)\n",
    "            if m[i][j] != 0:\n",
    "                continue\n",
    "            if i == j:\n",
    "                continue\n",
    "            # calculate H(i|c)\n",
    "            f_i_c = attr_factor[i].copy()\n",
    "            f_i_c.table = -f_i_c.table * np.log2(f_i_c.table)\n",
    "            f_i_c = f_i_c.join(prior)\n",
    "            H_i_c = np.sum(f_i_c.table)\n",
    "\n",
    "            # calculate H(i|j,c)\n",
    "            f_i_jc = estimateFactor_alpha(data,i, [j, var], outcomeSpace,alpha=alpha)\n",
    "            f_i_jc.table = -f_i_jc.table * np.log2(f_i_jc.table)\n",
    "            jc_prior = attr_factor[j].copy().join(prior)\n",
    "            f_i_jc = f_i_jc.join(jc_prior)\n",
    "            H_i_jc = np.sum(f_i_jc.table)\n",
    "\n",
    "            # calculate MI(i;j|c) = H(i|c) - H(i|j,c)\n",
    "            m[i][j] = H_i_c - H_i_jc\n",
    "            # this matrix must be symmetric\n",
    "            m[j][i] = m[i][j]\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(Graph):\n",
    "    # modified version of tut code to generate max spanning tree\n",
    "    def prim_max(self, start):\n",
    "        \"\"\"\n",
    "        argument \n",
    "        `start`, start vertex\n",
    "        \"\"\"      \n",
    "        visited = {start}\n",
    "        Q = []\n",
    "        tree = Graph()\n",
    "        for e in self.adj_list[start]:\n",
    "            pq.heappush(Q, (-1*self.edge_weights[(start,e)], start, e))\n",
    "        while len(Q) > 0:\n",
    "            weight, v, u = pq.heappop(Q)\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                tree.add_edge(v, u, weight=weight)\n",
    "                for e in self.adj_list[u]:\n",
    "                    if e not in visited:\n",
    "                        pq.heappush(Q, (-1*self.edge_weights[(u,e)], u, e))        \n",
    "        return tree\n",
    "\n",
    "\n",
    "\n",
    "def complete_undirected_graph(g_weights, attributes):\n",
    "\n",
    "    g = Graph()\n",
    "    for i in attributes:\n",
    "        g.add_node(i)\n",
    "\n",
    "    for i in attributes:\n",
    "        for j in attributes:\n",
    "            if i == j:\n",
    "                continue\n",
    "            g.add_edge(i,j,weight=g_weights[i][j],directed=False)\n",
    "    return g\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_tan_structure(data, class_var='BC'):\n",
    "    '''\n",
    "    Arguments:\n",
    "        data: a dataframe\n",
    "        class_var: The variable you will be classifying with this graph structure\n",
    "    Return:\n",
    "        graph: A Graph object\n",
    "    '''\n",
    "    attributes = list(outcomeSpace.keys())\n",
    "    attributes.remove(class_var)\n",
    "\n",
    "    g_weights = get_MI(data, class_var, outcomeSpace, attributes)\n",
    "    G = complete_undirected_graph(g_weights, attributes)\n",
    "\n",
    "    root = np.random.randint(0,high=len(attributes))\n",
    "    # select an arbitrary node as root\n",
    "    G_T = G.prim_max(attributes[root])\n",
    "    G_T.add_node(class_var)\n",
    "    for node in attributes:\n",
    "        G_T.add_edge(class_var, node)\n",
    "    return G_T\n",
    "\n",
    "def cv_tan(dataframe, var='BC', k=10):\n",
    "    accuracy_list = []\n",
    "    # suffule the data\n",
    "    dataframe = dataframe.sample(frac=1)\n",
    "    for i in range(k):\n",
    "        # split dataset into train and test\n",
    "        start = int(i*dataframe.shape[0]/k)\n",
    "        end = int((i+1)*dataframe.shape[0]/k)\n",
    "        train = dataframe.drop([i for i in range(start, end)], axis=0)\n",
    "        test = dataframe.iloc[start:end]\n",
    "        train = train.reset_index()\n",
    "        test = test.reset_index()\n",
    "        train.drop(columns=[\"index\"])\n",
    "        test.drop(columns=[\"index\"])\n",
    "\n",
    "        # train a model\n",
    "        g = learn_tan_structure(train, var)\n",
    "        model = BayesNet(graph=g, outcomeSpace=outcomeSpace)\n",
    "        model.learn_parameters(train, alpha=1)\n",
    "\n",
    "        # test the model with assess_bayes_net\n",
    "        acc = assess_bayes_net(model, test, var)\n",
    "        \n",
    "        accuracy_list.append(acc)\n",
    "    return accuracy_list\n",
    "\n",
    "def cross_validation_tan(dataframe, var='BC', k=10):\n",
    "    accuracy_list = cv_tan(dataframe, var, k)\n",
    "    return np.mean(accuracy_list), np.std(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test case\n",
      "Passed test case\n",
      "Passed test case\n"
     ]
    }
   ],
   "source": [
    "# ############\n",
    "# ## TEST CODE\n",
    "\n",
    "tan_graph = learn_tan_structure(data)\n",
    "test(len(tan_graph.children('BC')) == len(tan_graph)-1)\n",
    "test('FibrTissueDev' in tan_graph.children('Spiculation') or 'Spiculation' in tan_graph.children('FibrTissueDev'))\n",
    "\n",
    "# my evaluation\n",
    "acc, std = cross_validation_tan(data)\n",
    "test(abs(acc - 0.85) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [20 Marks] Task 6 - Report\n",
    "\n",
    "Write a report (**with less than 500 words**) summarising your findings in this assignment. Your report should address the following:\n",
    "\n",
    "a. Make a summary and discussion of the experimental results. You can analyse your results from different aspects such as accuracy, runtime, coding complexity and independence assumptions. You can use plots to illustrate your results.\n",
    "\n",
    "b. Discuss the time and memory complexity of the implemented algorithms.\n",
    "\n",
    "Use Markdown and Latex to write your report in the Jupyter notebook. Develop some plots using Matplotlib to illustrate your results. Be mindful of the maximum number of words. Please, be concise and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################################\n",
    "# plot generator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    bayes_acc = cv_bayes(data)\n",
    "    naive_acc = cv_naive(data)\n",
    "    tan_acc = cv_tan(data)\n",
    "    fig = plt.figure()\n",
    "    plt.boxplot([bayes_acc, naive_acc, tan_acc], labels=['Bayes', 'Naive Bayes', 'TAN'])\n",
    "    plt.savefig('imgs/result.jpg')\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Task 1 - Efficient d-seperation test\n",
    "a. Analysis: We add few extra tests that covers $dsep_G(X,Y,Z)$ in the cases of sequential, divergent and convergent. These tests proves the correctness of our implementation. Run 10 test cases will take around $0.7 seconds$ which is $0.07 s/run$.\n",
    "\n",
    "b. Complexity:\n",
    "- Time: $V$ stands for number of vertex, $E$ stands for number of edges. Repeatedly pruning nodes can be done in $O(V)$ and pruning edges can be done in constant time $O(1)$. The time complexity for DFS is $O(V+E)$.\n",
    "- Space: We make another undirected copy of pruned graph will would need extra memory $O(V+E)$. Luckily, the graph after pruning would be relatively smaller. During DFS, one color map of all vertex is created $O(V)$. \n",
    "\n",
    "## Task 2 – Estimate Bayesian Network parameters from data\n",
    "a. Analysis: For each node in graph, we need to go through the table to construct the CPT table. Thanks for the efficiency of Pandas dataframe, the entire process can be done in $0.1s$.\n",
    "\n",
    "b. Complexity: \n",
    "- Time: $O(VD)$ where $D$ is number of data\n",
    "- Space: $O(VEO)$ where $O$ is number of entries in outcomeSpace\n",
    "\n",
    "\n",
    "## Task 3 - Bayesian Network Classification \n",
    "\n",
    "a. Analysis: It takes 38.2 seconds to run the cross validation process with accuracy around 0.835. The graph is based on the causality proofed by experts which indicates the real independency between attributes.\n",
    "\n",
    "b. Complexity:\n",
    "- The time complexity of assess function n(sample size) times the complexity of query(). Which the complexity of cross validation is k times that of assess function.\n",
    "- The space taken are simply linear to the graph and dataset.\n",
    "\n",
    "## Task 4 - Naive bayes\n",
    "\n",
    "a. Analysis:\n",
    "By assuming attributes to be mutually independent with given target, naive bayes reduces the complexity of bayes net in significantly. It completed the classification task in 30 seconds which is 8 seconds less than that of the previous method. Even the accuracy is a little bit lower which is around 0.8 because of it's assumption on independence, it is still a good choice when dealing with small dataset with not significant internal correlation.\n",
    "\n",
    "b. Complexity:\n",
    "- The estimated time complexity of calculating $P(x|y_i)$ is $O(\\Pi _i^N s_i)$ where $N$ is the number of attributes and $s_i$ represents the number of possible values of nth variable.\n",
    "- The space complexity is similar to Bayes Net\n",
    "\n",
    "## Task 5 - Tree-augmented Naive (TAN) Bayes Classification\n",
    "a. Analysis:\n",
    "During the process of calculating $MI(i;j|c)$ we store each $P(i|c)$ to avoid duplicate estimate factor. We store all MI data in a dict(dict) to reduce look up time to constant time.\n",
    "\n",
    "b. Complexity:\n",
    "The time and space complexity of calculating MI is $O(N^2)$ while $N$ is number of attributes. Complete_undirected_graph has time complexity of $O(V^2)$ with space $O(VE)$. Time complexity of Prim algorithm is $O(E\\log{V})$. Adding query node to graph takes time of $O(V)$. The rest is essentially the same as Bayes Net.\n",
    "\n",
    "## Conclusion\n",
    "![](imgs/result.jpg)\n",
    "\n",
    "From the boxplot above, we can see that the graph learnt by expert (Bayes Net) performed the best. The graph learnt from data using TAN outperforms that of Naive Bayes since it takes the dependencies between attributes into consideration. Furthermore, the standard deviation of Naive Bayes is the largest since it assumes the attributes are independent given the class\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6714646c7badaf138256cae83466294356a604d5a9644f8ff87d802670eeaa68"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('vision': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
